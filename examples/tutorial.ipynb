{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO11 Tutorial",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simbakm/training_yolov11_notebook/blob/main/examples/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "# **starting Point @ simbakm Training YOlo  version 11 bold text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ff52e-2f95-4d38-f401-06be96538d8f"
      },
      "source": [
        "!uv pip install ultralytics\n",
        "!pip install roboflow kagglehub\n",
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 41.8/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2 â€” Download the Roboflow dataset (YOLOv11-ready)bold text**"
      ],
      "metadata": {
        "id": "8I6sumtzqek_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**helper fuction to couint the number of files**"
      ],
      "metadata": {
        "id": "5oUlOF0jjOUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def count_file_types(dataset_path):\n",
        "    file_counts = defaultdict(int)\n",
        "\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            ext = os.path.splitext(file)[1].lower()  # get file extension\n",
        "            file_counts[ext] += 1\n",
        "\n",
        "    print(\"File type counts:\\n\")\n",
        "    for ext, count in sorted(file_counts.items()):\n",
        "        print(f\"{ext if ext else 'No Extension'} : {count}\")\n",
        "\n",
        "    return dict(file_counts)\n"
      ],
      "metadata": {
        "id": "E-Ssey6njIMB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YdrZFjwWblcZBkXrqqaR\")\n",
        "project = rf.workspace(\"stairs-ttqqp\").project(\"trash-bin-asn0s\")\n",
        "dataset = project.version(1).download(\"yolov11\")\n",
        "\n",
        "roboflow_path = dataset.location\n",
        "print(\"Roboflow dataset:\", roboflow_path)\n",
        "\n",
        "counts = count_file_types(roboflow_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwsMXa8YnYMe",
        "outputId": "fd0de73d-75c0-47dd-fde6-8595d300d348"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in trash-bin-1 to yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21999/21999 [00:00<00:00, 33454.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to trash-bin-1 in yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 902/902 [00:00<00:00, 8563.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roboflow dataset: /content/trash-bin-1\n",
            "File type counts:\n",
            "\n",
            ".jpg : 445\n",
            ".txt : 447\n",
            ".yaml : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 3: Download the TACO dataset from KaggleHub**"
      ],
      "metadata": {
        "id": "xGzsfpkwp2mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"kneroma/tacotrashdataset\")\n",
        "print(\"Downloaded to:\", path)\n",
        "counts = count_file_types(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS_K-gNBpbZ8",
        "outputId": "e0f8e6be-a782-4792-ad74-9c3d58f531a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kneroma/tacotrashdataset?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.79G/2.79G [00:30<00:00, 97.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded to: /root/.cache/kagglehub/datasets/kneroma/tacotrashdataset/versions/3\n",
            "File type counts:\n",
            "\n",
            ".bin : 1\n",
            ".csv : 1\n",
            ".jpg : 1500\n",
            ".json : 1\n",
            ".txt : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 4 â€” Convert TACO (COCO JSON) â†’ YOLO format**"
      ],
      "metadata": {
        "id": "MAU8pdKBq3E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os, shutil\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# TACO dataset path (from KaggleHub)\n",
        "taco_path = path\n",
        "data_dir = os.path.join(taco_path, \"data\")\n",
        "\n",
        "# Load annotations\n",
        "annotations_path = os.path.join(data_dir, \"annotations.json\")\n",
        "annotations = json.load(open(annotations_path))\n",
        "\n",
        "# Output\n",
        "output_taco = \"/content/taco_yolo\"\n",
        "#delete privous derectories\n",
        "if os.path.exists(output_taco):\n",
        "    shutil.rmtree(output_taco)\n",
        "    print(\"Old taco_yolo directory removed.\")\n",
        "else:\n",
        "    print(\"No previous taco_yolo folder found.\")\n",
        "\n",
        "os.makedirs(f\"{output_taco}/images\", exist_ok=True)\n",
        "os.makedirs(f\"{output_taco}/labels\", exist_ok=True)\n",
        "\n",
        "# Map image id â†’ image info\n",
        "image_map = {img[\"id\"]: img for img in annotations[\"images\"]}\n",
        "\n",
        "# Find all images recursively\n",
        "all_images = {}\n",
        "image_files = glob(f\"{data_dir}/**/*.jpg\", recursive=True) + \\\n",
        "              glob(f\"{data_dir}/**/*.JPG\", recursive=True)\n",
        "\n",
        "for f in image_files:\n",
        "    all_images[os.path.relpath(f, data_dir)] = f\n",
        "\n",
        "\n",
        "print(\"Total images found:\", len(all_images))\n",
        "\n",
        "# Group annotations by image ID\n",
        "ann_by_image = {}\n",
        "for ann in annotations[\"annotations\"]:\n",
        "    img_id = ann[\"image_id\"]\n",
        "    ann_by_image.setdefault(img_id, []).append(ann)\n",
        "\n",
        "# Convert to YOLO\n",
        "for img_id, img_info in tqdm(image_map.items()):\n",
        "    filename = img_info[\"file_name\"]\n",
        "    batch_relative_path = filename  # contains batch_x/000000.jpg\n",
        "\n",
        "    if batch_relative_path not in all_images:\n",
        "        print(\"Missing image:\", batch_relative_path)\n",
        "        continue\n",
        "\n",
        "    src = all_images[batch_relative_path]\n",
        "\n",
        "    # create unique new filename based on img_id\n",
        "    new_name = f\"{img_id}.jpg\"\n",
        "    dst = f\"{output_taco}/images/{new_name}\"\n",
        "\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "    # image width/height\n",
        "    W = img_info[\"width\"]\n",
        "    H = img_info[\"height\"]\n",
        "\n",
        "    # label path (same name as image)\n",
        "    label_path = f\"{output_taco}/labels/{img_id}.txt\"\n",
        "    with open(label_path, \"w\") as f:\n",
        "        for ann in ann_by_image.get(img_id, []):\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "\n",
        "            # convert to YOLO\n",
        "            xc = (x + w/2) / W\n",
        "            yc = (y + h/2) / H\n",
        "            w /= W\n",
        "            h /= H\n",
        "            # Create category id mapping\n",
        "            categories = annotations[\"categories\"]\n",
        "            cat_id_map = {cat[\"id\"]: i for i, cat in enumerate(categories)}\n",
        "            class_id = cat_id_map[ann[\"category_id\"]]\n",
        "\n",
        "            f.write(f\"{class_id} {xc} {yc} {w} {h}\\n\")\n",
        "\n",
        "print(\"DONE â€” TACO converted to YOLO:\", output_taco)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujxSaGGlq8LN",
        "outputId": "3e11f701-4467-47ba-f8d7-80cb8000ef20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous taco_yolo folder found.\n",
            "Total images found: 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:13<00:00, 113.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE â€” TACO converted to YOLO: /content/taco_yolo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5: Split the dataset into Train,val and test using ratio 80|10|10**"
      ],
      "metadata": {
        "id": "2Vg63xcK29RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, shutil\n",
        "from glob import glob\n",
        "\n",
        "taco_img = \"/content/taco_yolo/images\"\n",
        "taco_lbl = \"/content/taco_yolo/labels\"\n",
        "\n",
        "images = sorted(glob(f\"{taco_img}/*.jpg\"))\n",
        "random.shuffle(images)\n",
        "\n",
        "n = len(images)\n",
        "train_split = int(n * 0.8)\n",
        "val_split = int(n * 0.9)\n",
        "\n",
        "train_files = images[:train_split]\n",
        "val_files   = images[train_split:val_split]\n",
        "test_files  = images[val_split:]\n",
        "\n",
        "def copy_split(files, folder):\n",
        "    os.makedirs(f\"/content/taco_split/images/{folder}\", exist_ok=True)\n",
        "    os.makedirs(f\"/content/taco_split/labels/{folder}\", exist_ok=True)\n",
        "    for img in files:\n",
        "        lbl = img.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
        "        shutil.copy(img, f\"/content/taco_split/images/{folder}/\")\n",
        "        shutil.copy(lbl, f\"/content/taco_split/labels/{folder}/\")\n",
        "\n",
        "copy_split(train_files, \"train\")\n",
        "copy_split(val_files, \"val\")\n",
        "copy_split(test_files, \"test\")\n",
        "\n",
        "print(\"TACO split complete!\")\n"
      ],
      "metadata": {
        "id": "6y816cQX26gg",
        "outputId": "7d6b91b7-a0fd-49da-b27a-39c8aafeb80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TACO split complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 6 â€” Create a combined dataset**"
      ],
      "metadata": {
        "id": "kWvTXXoIEOOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "rf_labels = glob.glob(f\"{roboflow_path}/**/labels/*.txt\", recursive=True)\n",
        "\n",
        "for file in rf_labels:\n",
        "    lines = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) > 0:\n",
        "                parts[0] = \"60\"  # move to class 60\n",
        "                lines.append(\" \".join(parts))\n",
        "\n",
        "    with open(file, \"w\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "print(\"Roboflow labels shifted to class 60\")\n"
      ],
      "metadata": {
        "id": "b9KdLht_q48H",
        "outputId": "566c7600-aa51-45bd-bd3b-9fbacf9ecc3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roboflow labels shifted to class 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove old combined folder\n",
        "shutil.rmtree(\"/content/combined\", ignore_errors=True)\n",
        "\n",
        "# create combined structure\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    os.makedirs(f\"/content/combined/images/{split}\", exist_ok=True)\n",
        "    os.makedirs(f\"/content/combined/labels/{split}\", exist_ok=True)\n",
        "\n",
        "# copy TACO train/val/test\n",
        "!cp -r /content/taco_split/images/train/* /content/combined/images/train/\n",
        "!cp -r /content/taco_split/labels/train/* /content/combined/labels/train/\n",
        "\n",
        "!cp -r /content/taco_split/images/val/* /content/combined/images/val/\n",
        "!cp -r /content/taco_split/labels/val/* /content/combined/labels/val/\n",
        "\n",
        "!cp -r /content/taco_split/images/test/* /content/combined/images/test/\n",
        "!cp -r /content/taco_split/labels/test/* /content/combined/labels/test/\n",
        "\n",
        "# copy roboflow\n",
        "# train\n",
        "!cp -r $roboflow_path/train/images/* /content/combined/images/train/\n",
        "!cp -r $roboflow_path/train/labels/* /content/combined/labels/train/\n",
        "\n",
        "# val\n",
        "!cp -r $roboflow_path/valid/images/* /content/combined/images/val/\n",
        "!cp -r $roboflow_path/valid/labels/* /content/combined/labels/val/\n",
        "\n",
        "# test\n",
        "!cp -r $roboflow_path/test/images/* /content/combined/images/test/\n",
        "!cp -r $roboflow_path/test/labels/* /content/combined/labels/test/\n",
        "\n"
      ],
      "metadata": {
        "id": "mPz4EvEKFYsN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step7: Create YOLO data.yaml**"
      ],
      "metadata": {
        "id": "HmgOeBR75SDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_yaml = \"\"\"\n",
        "train: /content/combined/images/train\n",
        "val: /content/combined/images/val\n",
        "test: /content/combined/images/test\n",
        "\n",
        "nc: 61\n",
        "names:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "03jXia1KqUSe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TACO categories\n",
        "import json\n",
        "annotations_path = os.path.join(path, \"data\", \"annotations.json\")\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "category_names = [cat[\"name\"] for cat in annotations[\"categories\"]]\n",
        "\n",
        "# Add trash_bin as last class\n",
        "category_names.append(\"trash_bin\")\n",
        "\n",
        "# Write YAML\n",
        "with open(\"combined.yaml\", \"w\") as f:\n",
        "    f.write(\"train: /content/combined/images/train\\n\")\n",
        "    f.write(\"val: /content/combined/images/val\\n\")\n",
        "    f.write(\"test: /content/combined/images/test\\n\\n\")\n",
        "    f.write(\"nc: 61\\n\")\n",
        "    f.write(\"names:\\n\")\n",
        "    for name in category_names:\n",
        "        f.write(f\"  - {name}\\n\")\n",
        "\n",
        "print(\"combined.yaml created correctly with 61 classes.\")\n"
      ],
      "metadata": {
        "id": "BnW14gs6qZxd",
        "outputId": "9c278994-a9e3-4697-95cc-f0cc505c8d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combined.yaml created correctly with 61 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8: â€” Train YOLOv11**"
      ],
      "metadata": {
        "id": "kCuUDEdq5j1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo train model=yolo11n.pt data=combined.yaml epochs=50 imgsz=640\n"
      ],
      "metadata": {
        "id": "tij-K73T5tNg",
        "outputId": "3159e766-436a-4f11-fe69-6d14908a5d30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 113.6MB/s 0.0s\n",
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=combined.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 25.2MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=61\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    442567  ultralytics.nn.modules.head.Detect           [61, 16, None, [64, 128, 256]]\n",
            "YOLO11n summary: 182 layers, 2,601,735 parameters, 2,601,719 gradients, 6.5 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.3MB 88.7MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3966.0Â±1038.9 MB/s, size: 718.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined/labels/train... 1514 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1514/1514 2.2Kit/s 0.7s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/combined/labels/train.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 167, len(boxes) = 4131. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 493.6Â±191.9 MB/s, size: 463.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined/labels/val... 255 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 255/255 1.0Kit/s 0.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/combined/labels/val.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 59, len(boxes) = 613. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000154, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      2.72G      1.105      5.186      1.188         47        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2s/it 1:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.6s/it 20.7s\n",
            "                   all        255        613     0.0254     0.0124     0.0175     0.0152\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50      3.16G      1.111      4.711       1.15         50        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.3it/s 1:15\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.7it/s 2.9s\n",
            "                   all        255        613      0.405     0.0538     0.0464     0.0369\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      3.18G      1.147      4.188      1.176         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:20\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.1it/s 3.7s\n",
            "                   all        255        613      0.603     0.0609      0.051      0.041\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      3.19G      1.131      3.888      1.162         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.539     0.0842     0.0633     0.0482\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50       3.2G      1.122      3.574      1.153         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.9it/s 4.1s\n",
            "                   all        255        613      0.372      0.087      0.071     0.0578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      3.22G      1.101      3.409      1.145         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.9it/s 2.7s\n",
            "                   all        255        613      0.446      0.141     0.0923     0.0736\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      3.24G      1.105      3.309      1.155         42        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.3it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.3it/s 3.4s\n",
            "                   all        255        613       0.47      0.139     0.0947      0.078\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      3.25G      1.096      3.132      1.124         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.7it/s 4.7s\n",
            "                   all        255        613      0.388      0.163     0.0945     0.0773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      3.26G       1.03      3.024      1.109         93        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.3s\n",
            "                   all        255        613      0.354      0.168      0.115      0.094\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      3.28G      1.042      2.976      1.112         45        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.7it/s 3.0s\n",
            "                   all        255        613      0.385      0.158      0.119     0.0948\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50       3.3G      1.028      2.923      1.104         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.2it/s 3.6s\n",
            "                   all        255        613       0.34      0.172      0.136       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      3.31G      1.046      2.869      1.105         49        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.3s\n",
            "                   all        255        613      0.458      0.148      0.128      0.105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      3.32G      1.007      2.786      1.085         69        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.4s\n",
            "                   all        255        613      0.389      0.162      0.143      0.118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      3.34G      1.025      2.733      1.086         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.4s\n",
            "                   all        255        613      0.434      0.139      0.145      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      3.36G     0.9907      2.708       1.09         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.452      0.174      0.161      0.131\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      3.37G     0.9875      2.603       1.07         46        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.0it/s 4.1s\n",
            "                   all        255        613      0.399       0.19      0.179      0.153\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      3.38G      0.998      2.574      1.081         47        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.7it/s 2.9s\n",
            "                   all        255        613      0.374      0.182      0.183      0.151\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50       3.4G     0.9699      2.529      1.065         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.3it/s 3.5s\n",
            "                   all        255        613       0.37      0.214       0.19      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      3.41G     0.9822       2.52      1.068         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.379      0.234      0.198      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      3.43G      0.991      2.496      1.059         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.458      0.185      0.208      0.175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      3.44G     0.9671      2.431       1.05         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.9it/s 4.2s\n",
            "                   all        255        613      0.424      0.174      0.194      0.166\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      3.46G     0.9526      2.382      1.059         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.4s\n",
            "                   all        255        613      0.451      0.198      0.201      0.166\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      3.47G     0.9404      2.376      1.047         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.3it/s 3.5s\n",
            "                   all        255        613      0.291      0.224      0.195      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50      3.49G     0.9287      2.296      1.042         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.1it/s 3.8s\n",
            "                   all        255        613      0.332      0.238      0.202      0.171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50       3.5G     0.9513      2.326      1.049         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.4s\n",
            "                   all        255        613      0.444      0.173      0.212      0.181\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      3.52G     0.9501      2.248      1.044         45        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.6it/s 3.1s\n",
            "                   all        255        613       0.35      0.248       0.22      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      3.53G     0.9491      2.278       1.04         58        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.1it/s 3.9s\n",
            "                   all        255        613      0.415      0.213      0.215      0.183\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      3.55G     0.9374      2.215      1.044         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.3s\n",
            "                   all        255        613       0.29      0.258      0.219      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      3.56G     0.9034      2.213      1.026         51        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 3.0it/s 2.7s\n",
            "                   all        255        613      0.361      0.256      0.231      0.196\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      3.58G      0.923      2.203      1.046         78        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.552      0.208      0.237      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      3.59G     0.9173      2.123      1.033         57        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.6it/s 3.0s\n",
            "                   all        255        613      0.364      0.222      0.225      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      3.61G     0.9163       2.09       1.04         58        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.0it/s 4.0s\n",
            "                   all        255        613      0.464      0.237      0.233      0.195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      3.62G     0.8898      2.058      1.014         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.6it/s 3.1s\n",
            "                   all        255        613      0.328      0.279      0.237      0.197\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      3.64G     0.9132      2.063      1.018         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.439      0.226      0.238      0.195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      3.65G     0.8971       2.02      1.026         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.8it/s 4.6s\n",
            "                   all        255        613      0.584      0.202      0.229      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50      3.67G     0.9004      2.047      1.026         99        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.401      0.223      0.236      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      3.68G     0.8883      1.989      1.016         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613       0.48      0.222       0.23      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50       3.7G     0.8914      1.992      1.015         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.9it/s 4.2s\n",
            "                   all        255        613      0.463      0.209      0.228      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      3.71G     0.8937      1.967       1.02         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.8it/s 2.8s\n",
            "                   all        255        613      0.506      0.221      0.243        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50      3.73G     0.8775      1.953      1.016         58        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.466      0.223      0.241      0.198\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      3.74G     0.8272      2.253     0.9684         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.4s\n",
            "                   all        255        613      0.297      0.227      0.234      0.195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50      3.76G     0.8143      2.153     0.9587         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.2it/s 3.7s\n",
            "                   all        255        613      0.325      0.223      0.225      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      3.77G     0.7889      2.084     0.9392         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.5it/s 3.2s\n",
            "                   all        255        613      0.324      0.231      0.232      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      3.79G     0.7887       2.07     0.9482         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:20\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.1it/s 3.8s\n",
            "                   all        255        613      0.342      0.229      0.235      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50       3.8G     0.7884      2.065     0.9476         21        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.6it/s 3.1s\n",
            "                   all        255        613      0.376      0.231      0.236      0.196\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50      3.82G     0.7816      2.022     0.9445         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.9it/s 4.3s\n",
            "                   all        255        613      0.509      0.207      0.241      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      3.83G     0.7761      2.014     0.9492         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.3s\n",
            "                   all        255        613       0.31      0.269      0.239      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      3.85G     0.7951      2.011     0.9518         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:20\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.3it/s 3.4s\n",
            "                   all        255        613      0.362      0.254      0.247      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      3.86G     0.7734      1.993     0.9411         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.8it/s 2.8s\n",
            "                   all        255        613      0.395      0.229      0.239        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      3.88G     0.7765      2.015     0.9353         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 95/95 1.2it/s 1:18\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 2.4it/s 3.3s\n",
            "                   all        255        613      0.503      0.201      0.235      0.196\n",
            "\n",
            "50 epochs completed in 1.160 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "YOLO11n summary (fused): 101 layers, 2,594,047 parameters, 0 gradients, 6.4 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 1.6it/s 5.2s\n",
            "                   all        255        613      0.337      0.254      0.248      0.208\n",
            "        Aluminium foil          3          4      0.418        0.5      0.428      0.307\n",
            "   Carded blister pack          1          1          0          0          0          0\n",
            "  Other plastic bottle          9         11      0.501      0.182      0.279      0.273\n",
            "  Clear plastic bottle         23         27      0.463      0.593      0.594      0.468\n",
            "          Glass bottle          5          6      0.364      0.333      0.511      0.378\n",
            "    Plastic bottle cap         23         25      0.381       0.24      0.331      0.211\n",
            "      Metal bottle cap          4          4       0.26       0.75      0.746      0.721\n",
            "          Broken glass          3         29          1          0          0          0\n",
            "              Food Can          1          3      0.482          1      0.995      0.912\n",
            "               Aerosol          1          1          0          0      0.142      0.142\n",
            "             Drink can         15         17      0.318      0.706       0.45      0.288\n",
            "           Toilet tube          1          3          0          0          0          0\n",
            "          Other carton          6          6      0.219      0.167     0.0632     0.0418\n",
            "            Egg carton          1          1          1          0          0          0\n",
            "          Drink carton          2          2     0.0807      0.202      0.128      0.125\n",
            "     Corrugated carton          7         12      0.513      0.583      0.581      0.501\n",
            "           Meal carton          2          2          0          0     0.0558     0.0502\n",
            "             Paper cup          4          4      0.113       0.25      0.106      0.106\n",
            "Disposable plastic cup          3          4          0          0     0.0144     0.0126\n",
            "           Plastic lid          4          4          0          0      0.013     0.0107\n",
            "         Other plastic         17         34      0.379      0.118     0.0963      0.082\n",
            "               Tissues          4          5          0          0     0.0193      0.018\n",
            "        Wrapping paper          2          3          1          0     0.0281     0.0281\n",
            "          Normal paper          5          6          0          0     0.0421     0.0354\n",
            "             Paper bag          4          5      0.363        0.4      0.458      0.347\n",
            "          Plastic film         24         27      0.315      0.444      0.253      0.195\n",
            "           Garbage bag          2          2       0.18       0.27      0.166      0.149\n",
            " Other plastic wrapper         18         19      0.448      0.474      0.552      0.474\n",
            "Single-use carrier bag          3          9      0.184      0.111      0.159      0.102\n",
            "          Crisp packet          3          3      0.208      0.333       0.13     0.0595\n",
            "Disposable food container          4          4      0.267       0.25      0.272      0.268\n",
            "   Foam food container          1          1       0.62          1      0.995      0.995\n",
            "Other plastic container          1          1          1          0          0          0\n",
            "      Plastic utensils          2          2          0          0     0.0442     0.0134\n",
            "               Pop tab          5          6      0.328      0.333      0.313      0.231\n",
            "        Rope & strings          5          5          0          0          0          0\n",
            "           Scrap metal          2          2          1          0          0          0\n",
            "                  Shoe          3          3          0          0          0          0\n",
            "         Plastic straw          8          8      0.448      0.375      0.408      0.291\n",
            "       Styrofoam piece         11         13      0.277      0.308      0.257      0.243\n",
            "      Unlabeled litter         23         46      0.169     0.0652     0.0791      0.047\n",
            "             Cigarette         23        104      0.313     0.0288     0.0274     0.0149\n",
            "             trash_bin        105        139        0.9      0.908      0.945        0.8\n",
            "Speed: 0.3ms preprocess, 2.5ms inference, 0.0ms loss, 5.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 6 â€” Download trained model**"
      ],
      "metadata": {
        "id": "qQ5kBw2S6Uzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/runs/detect/train/weights/best.pt\")\n"
      ],
      "metadata": {
        "id": "scuFucvT6kht",
        "outputId": "adb0cd19-49a4-491d-b6ba-d7514f6fe79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bafc10fc-6b84-4905-9e85-17ca2b57d843\", \"best.pt\", 5483162)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Predict\n",
        "\n",
        "YOLO11 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLO11 Predict Docs](https://docs.ultralytics.com/modes/train/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d438406-032a-4ea9-9ad5-dd17078850d3"
      },
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=yolo11n.pt source='https://ultralytics.com/images/zidane.jpg'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4/5.4MB 67.6MB/s 0.1s\rDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4/5.4MB 67.5MB/s 0.1s\n",
            "Ultralytics 8.3.187 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "Downloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg': 100% â”â”â”â”â”â”â”â”â”â”â”â” 49.2/49.2KB 5.9MB/s 0.0s\n",
            "image 1/1 /content/zidane.jpg: 384x640 2 persons, 1 tie, 68.9ms\n",
            "Speed: 13.1ms preprocess, 68.9ms inference, 250.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/212889447-69e5bdf1-5800-4e29-835e-2ed2336dede2.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Val\n",
        "Validate a model's accuracy on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset's `val` or `test` splits. The latest YOLO11 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLO11 Val Docs](https://docs.ultralytics.com/modes/val/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_"
      },
      "source": [
        "# Download COCO val\n",
        "from ultralytics.utils.downloads import download\n",
        "\n",
        "download('https://ultralytics.com/assets/coco2017val.zip', unzip=True, dir='datasets') # download (780MB - 5000 images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH",
        "outputId": "27364fde-3aff-47ea-9458-18e4a044e27b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Validate YOLO11n on COCO8 val\n",
        "!yolo val model=yolo11n.pt data=coco8.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.187 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "WARNING âš ï¸ Dataset 'coco8.yaml' images not found, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip': 100% â”â”â”â”â”â”â”â”â”â”â”â” 432.8/432.8KB 12.2MB/s 0.0s\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100% â”â”â”â”â”â”â”â”â”â”â”â” 25/25 4962.5files/s 0.0s\n",
            "Dataset download success âœ… (0.6s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1/755.1KB 22.0MB/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1486.2Â±504.6 MB/s, size: 54.0 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 4/4 118.2it/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.88it/s 1.1s\n",
            "                   all          4         17       0.57       0.85      0.847      0.632\n",
            "                person          3         10      0.557        0.6      0.585      0.272\n",
            "                   dog          1          1      0.548          1      0.995      0.697\n",
            "                 horse          1          2      0.531          1      0.995      0.674\n",
            "              elephant          1          2      0.371        0.5      0.516      0.256\n",
            "              umbrella          1          1      0.569          1      0.995      0.995\n",
            "          potted plant          1          1      0.847          1      0.995      0.895\n",
            "Speed: 0.3ms preprocess, 18.1ms inference, 0.0ms loss, 21.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://ultralytics.com/hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n",
        "\n",
        "Train YOLO11 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLO11 Train Docs](https://docs.ultralytics.com/modes/train/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select YOLO11 ğŸš€ logger {run: 'auto'}\n",
        "logger = 'TensorBoard' #@param ['TensorBoard', 'Weights & Biases']\n",
        "\n",
        "if logger == 'TensorBoard':\n",
        "  !yolo settings tensorboard=True\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir .\n",
        "elif logger == 'Weights & Biases':\n",
        "  !yolo settings wandb=True"
      ],
      "metadata": {
        "id": "ktegpM42AooT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "outputId": "849c2875-cd29-4a93-a7c7-e464a9b84dc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train YOLO11n on COCO8 for 3 epochs\n",
        "!yolo train model=yolo11n.pt data=coco8.yaml epochs=3 imgsz=640"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.187 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
            "\n",
            "Transferred 499/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1134.4Â±454.3 MB/s, size: 50.0 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 4/4 1726.9it/s 0.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 446.6Â±70.7 MB/s, size: 54.0 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 4/4 27458.6it/s 0.0s\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3     0.645G     0.9078       2.63      1.334         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.09it/s 11.5s\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.30it/s 3.3s\n",
            "                   all          4         17      0.592       0.85      0.878      0.634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3     0.674G      1.328      3.181      1.625         46        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.7it/s 0.1s\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.6it/s 0.1s\n",
            "                   all          4         17      0.584       0.85      0.847      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3     0.674G     0.9821      3.036      1.326         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.9it/s 0.1s\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 14.3it/s 0.1s\n",
            "                   all          4         17      0.587       0.85      0.851      0.615\n",
            "\n",
            "3 epochs completed in 0.004 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.187 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 25.5it/s 0.0s\n",
            "                   all          4         17      0.592       0.85      0.877      0.633\n",
            "                person          3         10      0.586        0.6      0.585      0.262\n",
            "                   dog          1          1      0.554          1      0.995      0.697\n",
            "                 horse          1          2      0.616          1      0.995      0.674\n",
            "              elephant          1          2      0.373        0.5      0.695      0.275\n",
            "              umbrella          1          1      0.573          1      0.995      0.995\n",
            "          potted plant          1          1      0.854          1      0.995      0.895\n",
            "Speed: 0.3ms preprocess, 4.3ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLO model to any supported format below with the `format` argument, i.e. `format=onnx`. See [Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- ğŸ’¡ ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.\n",
        "- ğŸ’¡ ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format | `format` Argument | Model | Metadata | Arguments |\n",
        "|--------|-----------------|-------|----------|------------|\n",
        "| [PyTorch](https://pytorch.org/) | - | `yolo11n.pt` | âœ… | - |\n",
        "| [TorchScript](https://docs.ultralytics.com/integrations/torchscript) | `torchscript` | `yolo11n.torchscript` | âœ… | `imgsz`, `batch`, `dynamic`, `optimize`, `half`, `nms`, `device` |\n",
        "| [ONNX](https://docs.ultralytics.com/integrations/onnx) | `onnx` | `yolo11n.onnx` | âœ… | `imgsz`, `batch`, `dynamic`, `half`, `opset`, `simplify`, `nms`, `device` |\n",
        "| [OpenVINO](https://docs.ultralytics.com/integrations/openvino) | `openvino` | `yolo11n_openvino_model/` | âœ… | `imgsz`, `batch`, `dynamic`, `half`, `int8`, `nms`, `fraction`, `device`, `data` |\n",
        "| [TensorRT](https://docs.ultralytics.com/integrations/tensorrt) | `engine` | `yolo11n.engine` | âœ… | `imgsz`, `batch`, `dynamic`, `half`, `int8`, `simplify`, `nms`, `fraction`, `device`, `data`, `workspace` |\n",
        "| [CoreML](https://docs.ultralytics.com/integrations/coreml) | `coreml` | `yolo11n.mlpackage` | âœ… | `imgsz`, `batch`, `half`, `int8`, `nms`, `device` |\n",
        "| [TF SavedModel](https://docs.ultralytics.com/integrations/tf-savedmodel) | `saved_model` | `yolo11n_saved_model/` | âœ… | `imgsz`, `batch`, `int8`, `keras`, `nms`, `device` |\n",
        "| [TF GraphDef](https://docs.ultralytics.com/integrations/tf-graphdef) | `pb` | `yolo11n.pb` | âŒ | `imgsz`, `batch`, `device` |\n",
        "| [TF Lite](https://docs.ultralytics.com/integrations/tflite) | `tflite` | `yolo11n.tflite` | âœ… | `imgsz`, `batch`, `half`, `int8`, `nms`, `fraction`, `device`, `data` |\n",
        "| [TF Edge TPU](https://docs.ultralytics.com/integrations/edge-tpu) | `edgetpu` | `yolo11n_edgetpu.tflite` | âœ… | `imgsz`, `device` |\n",
        "| [TF.js](https://docs.ultralytics.com/integrations/tfjs) | `tfjs` | `yolo11n_web_model/` | âœ… | `imgsz`, `batch`, `half`, `int8`, `nms`, `device` |\n",
        "| [PaddlePaddle](https://docs.ultralytics.com/integrations/paddlepaddle) | `paddle` | `yolo11n_paddle_model/` | âœ… | `imgsz`, `batch`, `device` |\n",
        "| [MNN](https://docs.ultralytics.com/integrations/mnn) | `mnn` | `yolo11n.mnn` | âœ… | `imgsz`, `batch`, `half`, `int8`, `device` |\n",
        "| [NCNN](https://docs.ultralytics.com/integrations/ncnn) | `ncnn` | `yolo11n_ncnn_model/` | âœ… | `imgsz`, `batch`, `half`, `device` |\n",
        "| [IMX500](https://docs.ultralytics.com/integrations/sony-imx500) | `imx` | `yolo11n_imx_model/` | âœ… | `imgsz`, `int8`, `fraction`, `device`, `data` |\n",
        "| [RKNN](https://docs.ultralytics.com/integrations/rockchip-rknn) | `rknn` | `yolo11n_rknn_model/` | âœ… | `imgsz`, `batch`, `name`, `device` || [ExecuTorch](https://docs.ultralytics.com/integrations/executorch) | `executorch` | `executorch_model/` | âœ… | `imgsz`, `device` |"
      ],
      "metadata": {
        "id": "nPZZeNrLCQG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=yolo11n.pt format=torchscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIjW4igCjqD",
        "outputId": "f06e7d97-01d9-45f4-b24b-e90b08291675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.187 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CPU (Intel Xeon 2.00GHz)\n",
            "ğŸ’¡ ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.8.0+cu126...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.1s, saved as 'yolo11n.torchscript' (10.5 MB)\n",
            "\n",
            "Export complete (2.6s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n.torchscript imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolo11n.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
            "Visualize:       https://netron.app\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLO11 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLO11 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLO11 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ],
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolo11n.yaml')  # build a new model from scratch\n",
        "model = YOLO('yolo11n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data='coco8.yaml', epochs=3)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set\n",
        "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
        "results = model.export(format='onnx')  # export the model to ONNX format"
      ],
      "metadata": {
        "id": "bpF9-vS_DAaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Tasks\n",
        "\n",
        "YOLO11 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLO11 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n",
        "\n",
        "<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\">\n"
      ],
      "metadata": {
        "id": "Phm9ccmOKye5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Detection\n",
        "\n",
        "YOLO11 _detection_ models have no suffix and are the default YOLO11 models, i.e. `yolo11n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n"
      ],
      "metadata": {
        "id": "yq26lwpYK1lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO11n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n.pt')  # load a pretrained YOLO detection model\n",
        "model.train(data='coco8.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "8Go5qqS9LbC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "YOLO11 _segmentation_ models use the `-seg` suffix, i.e. `yolo11n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n"
      ],
      "metadata": {
        "id": "7ZW58jUzK66B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO11n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n-seg.pt')  # load a pretrained YOLO segmentation model\n",
        "model.train(data='coco8-seg.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "WFPJIQl_L5HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Classification\n",
        "\n",
        "YOLO11 _classification_ models use the `-cls` suffix, i.e. `yolo11n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n"
      ],
      "metadata": {
        "id": "ax3p94VNK9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO11n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n-cls.pt')  # load a pretrained YOLO classification model\n",
        "model.train(data='mnist160', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "5q9Zu6zlL5rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pose\n",
        "\n",
        "YOLO11 _pose_ models use the `-pose` suffix, i.e. `yolo11n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details."
      ],
      "metadata": {
        "id": "SpIaFLiO11TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO11n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n-pose.pt')  # load a pretrained YOLO pose model\n",
        "model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "si4aKFNg19vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Oriented Bounding Boxes (OBB)\n",
        "\n",
        "YOLO11 _OBB_ models use the `-obb` suffix, i.e. `yolo11n-obb.pt` and are pretrained on the DOTA dataset. See [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for full details."
      ],
      "metadata": {
        "id": "cf5j_T9-B5F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO11n-obb, train it on DOTA8 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n-obb.pt')  # load a pretrained YOLO OBB model\n",
        "model.train(data='dota8.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/boats.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "IJNKClOOB5YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install from source\n",
        "!uv pip install git+https://github.com/ultralytics/ultralytics@main"
      ],
      "metadata": {
        "id": "pIdE6i8C3LYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Git clone and run tests on 'main' branch\n",
        "!git clone https://github.com/ultralytics/ultralytics -b main\n",
        "!uv pip install -qe ultralytics"
      ],
      "metadata": {
        "id": "uRKlwxSJdhd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run tests (Git clone only)\n",
        "!pytest ultralytics/tests"
      ],
      "metadata": {
        "id": "GtPlh7mcCGZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate multiple models\n",
        "for x in 'nsmlx':\n",
        "  !yolo val model=yolo11{x}.pt data=coco.yaml"
      ],
      "metadata": {
        "id": "Wdc6t_bfzDDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d86f508f"
      },
      "source": [
        "# Task\n",
        "The \"Missing image\" messages are not indicative of an error in the provided code. They appear because some image files referenced in the TACO dataset's `annotations.json` are not present in the downloaded `tacotrashdataset/data` directory. The script correctly handles this by skipping those annotations for which the corresponding image file is missing.\n",
        "\n",
        "If you intend to use all annotations, you might need to:\n",
        "1.  **Verify the integrity of the downloaded TACO dataset**: Ensure that the Kaggle dataset \"kneroma/tacotrashdataset\" you downloaded is complete and all referenced image files are present.\n",
        "2.  **Manually download missing images**: If there are specific images crucial for your task, you might need to locate and download them separately if they exist elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a6c4c10"
      },
      "source": [
        "## explain_missing_images\n",
        "\n",
        "### Subtask:\n",
        "Explain that the 'Missing image' messages are due to images being absent from the downloaded TACO dataset, not a code error. The script correctly handles this by skipping them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4376fda3"
      },
      "source": [
        "The 'Missing image' messages you observed in the previous output are not an error in the code's logic. They indicate that some image files referenced in the TACO dataset's 'annotations.json' are not actually present in the '/kaggle/input/tacotrashdataset/data' directory after downloading the dataset from KaggleHub. The script was designed to handle this by skipping the annotations associated with these missing images, allowing the conversion process to continue without crashing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f25584f0"
      },
      "source": [
        "## propose_dataset_integrity_check\n",
        "\n",
        "### Subtask:\n",
        "Suggest that the user might need to verify the integrity of the downloaded TACO dataset or ensure all necessary image files are present if they expect all annotations to have corresponding images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3285feda"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The 'Missing image' messages observed in the previous output are not indicative of an error in the provided code. Instead, they signify that certain image files referenced in the TACO dataset's `annotations.json` file are not present in the `/kaggle/input/tacotrashdataset/data` directory. The script is designed to handle this situation by skipping the annotations associated with these missing images, thus preventing crashes and allowing the conversion process to continue.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The 'Missing image' messages are a result of discrepancies between the `annotations.json` file and the actual image files available in the downloaded dataset.\n",
        "*   The script correctly identifies and skips annotations for which corresponding image files are absent, demonstrating robust error handling.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   If the goal is to utilize all annotations, it is crucial to verify the integrity and completeness of the downloaded TACO dataset from Kaggle to ensure all referenced image files are present.\n",
        "*   Alternatively, if specific missing images are essential, they may need to be located and downloaded manually from external sources.\n"
      ]
    }
  ]
}